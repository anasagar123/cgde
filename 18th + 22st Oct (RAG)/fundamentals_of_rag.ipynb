{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96fdaef-df89-4c0a-b297-046e5232671e",
   "metadata": {
    "id": "d96fdaef-df89-4c0a-b297-046e5232671e"
   },
   "source": [
    "# **Introduction to Retrieval Augmented Generation**\n",
    "\n",
    "We know that LLMs have the capability to generate stuff by themselves. But these tools aren't perfect.\n",
    "\n",
    "Even though they're super smart, they sometimes get things wrong, especially if they need to be really precise or use the latest information. So, to fix this, some of the brightest minds at Meta AI came up with a new trick called retrieval-augmented generation, or RAG for short, in 2020.\n",
    "\n",
    "Think of it as giving our language models an assistant. This assistant digs through a massive pile of updated information and feeds the most relevant and recent bits to the LLM.\n",
    "\n",
    "**Benefits:**  \n",
    "1. **Enhanced factual accuracy and Domain Specific Expertise:** Imagine a customer service chatbot trained on general conversation data. It might struggle with technical domain specific questions. RAGs allow you to integrate domain-specific knowledge bases, enabling the chatbot to handle these inquires with expertise.\n",
    "2. **Reduce Hallucination:** LLMs can generate false information, a phenomenon known as hallucination. The knowledge base provided can help support the claims of generative model.\n",
    "\n",
    "**Components of RAG:**  \n",
    "1. **Retrieval:** When a user asks a question or provides a prompt, retrievals first help fetch relevant passages from a vast knowledge base. This Knowledge Base could be the company's internal documents, or any other source of text data.\n",
    "2. **Augmentation:** The retrieved passages are then used to \"augment\" the LLM's knowledge. This can include various techniques, such as summarization or encoding the key information.\n",
    "3. **Generation:** Finally LLM leverages its understanding of language along with the augmented information to generate a response. This response can be an answer to a question, a creative text format based on a prompt, etc...\n",
    "\n",
    "**Applications:**  \n",
    "1. Question Answering: A RAG powered customer care chatbot can answer customer queries by retrieving product information, FAQs and guides to provide a well-rounded response.\n",
    "2. Document Summarization: A research paper summarization tool can use RAG to retrieve relevant sections and then generate a summary highlighting main points.\n",
    "3. Creative Text Generation: A story writing assistant can use RAG to retrieve information about historical periods or fictional creation, helping LLM to generate more deeply engaging stories.\n",
    "4. Code Generation: A code completion tool can use RAG to retrieve relevant code examples and API documentation, helping developers write code more efficiently.\n",
    "\n",
    "\n",
    "## **What are Retrievals?**\n",
    "Understand that the retrievals are specialized in navigating through vast amounts of data to find information that is relevant to a specific query or context.\n",
    "\n",
    "Retrieval models focus on the precision of matching query criteria with the data they have access to. Note that retrieval models rely heavily on the quality and structure of the data they access. Their performance depends on the relevance and accuracy of the information stored in the databases they query.\n",
    "\n",
    "In simple terms, retrievals search and identify relevant data from a large corpus for a given query.\n",
    "\n",
    "## **Building a RAG System**\n",
    "Step 1: Create an Index on available Knowledge Base  \n",
    "- Data from formats like PDF, HTML, etc is cleaned and converted into plain text. This text is then divided into smaller parts (i.e chunks) and turned into vector representations by passing the chunks into the embedding model to make it easier to find later.\n",
    "\n",
    "Step 2: Create a Retrival\n",
    "- When someone asks a question, the RAG system turns that question into vector embedding using the same method used in indexing. Then, it compares this vector to the vectors of the indexed text parts to fing the `k` most similar chunks. These `k` most similar chunks are used in the next step as a context.\n",
    "\n",
    "Step 3: Generation  \n",
    "- The system combines the retrieved text parts (i.e. context) with the original question to create a prompt. The LLM uses this prompt to answer the question.\n",
    "\n",
    "**Step 1: Create an Index on available Knowledge Base**      \n",
    "1. **Data Collection:** Carefully ingest the data from various sources. This data forms the basis of Knowledge Base.\n",
    "2. **Split and Parse:** Once the data is ingested, it needs to be broken down into manageable chunks. This is important because the LLMs havve a maximum context wondow that they can process in one go. During this step the data is not only splitted but also parsed to extract the useful metadata. Metadata can be information like document title, authors, etc...\n",
    "3. **Embedding Generation:** The next step is to convert the chunks into vector embeddings. This can be done using embedding models like BERT, GPT etc... that transforms text into a vector space while capturing semantic relationships and contextual meaning into numberical representation.\n",
    "4. **Vector Database:** The final step is to store the generated embeddings along with the metadata in a vector database such as ChromaDB, PineCone, etc... These databases are optimized for handing large volumes of data and allow efficient querying."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
