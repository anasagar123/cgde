# Apply LoRA Configuration
lora_config = LoraConfig(
    r=8,  
    lora_alpha=32,  
    lora_dropout=0.1,  
    task_type=TaskType.SEQ_CLS,  # Sequence Classification for Sentiment Analysis
)

LoraConfig(...)
---------------
LoraConfig → This is a class from the peft (Parameter-Efficient Fine-Tuning) library that allows us to define LoRA parameters.

r=8
------
r (Rank of LoRA adaptation matrix) defines the number of low-rank dimensions used in LoRA adaptation.
Higher r → More trainable parameters (better performance but more memory).
Lower r → Fewer trainable parameters (less memory but may reduce accuracy).
r=8 is a balance between efficiency and performance.

lora_alpha=32
----------------
lora_alpha is a scaling factor that controls the weight of LoRA's adapted layers in the model.

Higher lora_alpha increases the importance of LoRA-updated layers in the model’s output.
Lower values reduce their influence.
lora_alpha=32 is commonly used for effective fine-tuning.

lora_dropout=0.1
-----------------
lora_dropout is the dropout rate applied to LoRA layers during training.
Dropout helps prevent overfitting by randomly setting a fraction (10% here) of activations to zero during training.

task_type=TaskType.SEQ_CLS
-----------------------------
TaskType.SEQ_CLS tells LoRA that we are fine-tuning the model for Sequence Classification (SEQ_CLS).
Other possible values for task_type:
TaskType.CAUSAL_LM → For text generation (e.g., GPT models).
TaskType.TOKEN_CLS → For token classification (e.g., Named Entity Recognition - NER).
TaskType.SEQ_2_SEQ_LM → For sequence-to-sequence models (e.g., machine translation).


# Apply LoRA to BERT Model
model = get_peft_model(model, lora_config)
------------------------------------------
get_peft_model(model, lora_config)
get_peft_model(...) is a function from peft that applies LoRA modifications to our BERT model.
What it does:
Takes our base BERT model and injects LoRA adapters into it.
Instead of modifying the whole BERT model, it adds trainable low-rank adapters to specific layers.
This allows fine-tuning without modifying the entire model, saving memory.


