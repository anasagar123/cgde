ðŸ”¹ What is Fine Tuning?
ANS :
	---> Fine-tuning is the process of taking a pre-trained model(like bert,gpt etc) and training 
             it further on a specific  dataset for particular task or domain.
	---> like i want to build a chatbot for specific domain or particular task,and i am not getting good 
             result, i used prompt engineering even i am not getting good result then i have go with fine 
             tuning approach, i have to take dataset for specific domain or particular task and train dataset
             to pre-trained model. 

		ADVANTAGE OF FINE-TUNING :
	--> Improved Accuracy for Specific Tasks
	--> Better Performance with less data
	--> Domain Adaptation
	
		APPROACHES TO FINE-TUNING :

	  1. FULL FINE-TUNING ?

	---> Full fine-tuning refers to updating all the parameters of a pre-trained model during the fine-tuning process.
	---> Full fine-tuning is like bert model is there, it is using 120 million parameter if i will use full fine tuning
             approach, then it will update all the parameter of a pre-trained model (bert) during the fine-tuning process.which through 
             it wil require more gpu,memory.computationally it will expensive thats why industry is not using fine tuning, industry is 
             using Lora and qlora technique.

	2. PEFT(Parameter-Efficient Fine-tuning)
   
     ---> Peft is a framework or approach that focus on fine-tuning large models without updating all parameters.
     ---> Avoid full fine-tuning
     ---> only fine-tuning a small subset of parameters
     ---> Make training faster,cheaper,and more memory-efficient.
     ---> LoRA(Low-Rank Adaptation) is example of PEFT Methods.

	LoRA(Low-Rank Adaptation)

    ---> it will based on the decompose the matrix,like i have original matrix 3*3,so it will break the matrix 1*3 
         and 3*1 if i will do dot product of both i will get original matrix. so it will freeze all the parameter to 
         pre-trained model and i it will add only trainable parameter to the pre-trained model. that approch is called 
         Lora technique.

	HYPERPARAMETER :

	r=8 
    --> r(Rank of LoRA adaptation matrix) defines the number of low-rank dimensions used in LoRA adaptation.
    --> Higher r â†’ More trainable parameters (better performance but more memory).
    -->	Lower r â†’ Fewer trainable parameters (less memory but may reduce accuracy).
    -->	r=8 is a balance between efficiency and performance.

	lora_alpha=32
    --> lora_alpha is a scaling factor that controls the weight of LoRA's adapted layers in the model.
    --> lora_alpha=32 is commonly used for effective fine-tuning.

	lora_dropout=0.1
    --> Dropout helps prevent overfitting.

	task_type=TaskType.SEQ_CLS
   --> TaskType.SEQ_CLS tells LoRA that we are fine-tuning the model for Sequence Classification (SEQ_CLS).
   --> Other possible values for task_type:
   --> TaskType.CAUSAL_LM â†’ For text generation (e.g., GPT models).
   --> TaskType.TOKEN_CLS â†’ For token classification (e.g., Named Entity Recognition - NER).
   --> TaskType.SEQ_2_SEQ_LM â†’ For sequence-to-sequence models (e.g., machine translation).


	3. Qlora(Quantized Low-RANK Adaptation)

   --> Qlora is 4bit quantization.
   --> Lora is fast compare to Qlora because qlora firstly reduce the weight in 4bit then, it 
       will add trainable parameter to the pre-trained model.
   --> implementation wise will take same hyperparameter as lora only convert into a 4 bit weight.

------------------------------------------------------------------------------------------------------------------------------------------------------------   
	                                                   PROMPT ENGINEERING
                                                   ----------------------------------

ðŸ”¹ Prompt Engineering :
ANS :
	--> Prompt engineering/Designing is about providing input to the LLMs which can help 
            us desired results & improve the language model performance.

		BENEFITS OF PROMPT ENGINEERING :
	--> Improve model accuracy
	--> Guide the model to follow specific formats
	--> Reduce Hallucination
	--> Solve complex tasks without retraining the model.

	Types of Prompting Techniques :

	1. Zero-shot Prompting - just ask the question
				Example :
					  Translate this sentence to French : How are You?
	2. one-shot prompting - give one example
	3. few-shot prompting - provide multiple example which through model can understand context and style
	4. chain of thought (COT)   - ask the model to reason step-by-step
	5. tree of thought (TOT)   - complex problem-solving(math,planning,code,puzzales)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
							             





